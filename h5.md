# Project VirtualPrivateLab
**Description**
- A basic virtual private lab that you can easily set up and modify for your own purposes.

**Prerequisites**
- Computer
- Vagrant
- VirtualBox

# How to use
Navigate to your designated terminal and run   
```
$ git clone git@github.com:Ali-Mikael/VirtualPrivateLab.git
```
While inside the directory. Run: `vagrant up` <br> 
The repository can be found for inspection on <https://github.com/Ali-Mikael/VirtualPrivateLab> 
<br> 

# Breaking it down
**Step 1:** Created git repository `VirtualPrivateLab` on my computer. <br> 
**Step 2:** Created a vagrantfile and the salt directory. <br> 
**Step 3:** Configure, configure, configure.

#### Vagrant:
- Creates 1 `salt-master` & 3x `salt-minion`
- The minions are created inside a for loop for scalability
- Configures private networking.
  - Set Master IP-address = network prefix + `.100`
  - Minions will get = address prefix + `.10x`
- As this is a lab setup and not production, the master will auto-accept all minion keys.
- There is an option in the `vagrantfile` to use salt bootstrap for installation instead of the `apt` method. This is a lightweight bare down version of salt installation and many modules need manual installing if used.
- The VirtuaBox synced folder (using the VBGuestAdditions) initially caused me some problems. So I went ahead and disabled it completely and now using Vagrant's own `rsync`. More portable and robust this way. Only problem is you have to manually reboot with `$ vagrant reload master` to apply changes if modifying configs on your host computer. Not a huge problem tho and we've lived through worse! And besides, like I said, it's more robust this way, as it works better most of the times and better compatibility with different distributions/versions. 
  - Disable folder globally `config.vm.synced_folder ".", "/vagrant", disabled: true`
  - Using `config.vm.synced_folder "./files", "/home/vagrant/files", type: "rsync", rsync__auto: true` instead
  - The above mentioned folder will among other things be used to transfer the saltproject public key to the VMs.
  - Use rsync for /srv/salt & /srv/pillar
    - `master.vm.synced_folder "./salt", "/srv/salt", type: "rsync", rsync__auto: true`
    - `master.vm.synced_folder "./pillar", "/srv/pillar", type: "rsync", rsync__auto: true`
- The Vagrantfile is heavily commented for readability so the rest you can figure out from there!

The file in question <br> 

```
# -*- mode: ruby -*-
# vi: set ft=ruby :


# Basic setup script for all VMs
# ------------------------------
$initscript = <<-'INITSCRIPT'
  set -o verbose
  apt update && apt upgrade -y
  apt install -y curl tree gnupg
INITSCRIPT


# Install Salt
# ---------------

# Using bootstrap method
$salt_install_bootstrap = <<-'SCRIPT'
  # If using bootstrap method remove "apt install" from the Minion/Master script
  # See https://docs.saltproject.io/salt/install-guide/en/latest/ for more
  curl -o bootstrap-salt.sh -L https://github.com/saltstack/salt-bootstrap/releases/latest/download/bootstrap-salt.sh
  sh bootstrap-salt.sh -P
SCRIPT


# Using APT
$salt_install_apt = <<-'SCRIPT'
  # Ensure keyrings dir exists
  mkdir -p /etc/apt/keyrings

  # Copy SaltProject pub key to keyrings dir  
  cp /home/vagrant/files/SaltProjectKey.gpg.pub /etc/apt/keyrings/salt-archive-keyring.pgp

  # Create apt repo target configuration
  curl -fsSL https://github.com/saltstack/salt-install-guide/releases/latest/download/salt.sources | sudo tee /etc/apt/sources.list.d/salt.sources > /dev/null

  # Update metadata
  apt update
SCRIPT

# Salt-Minion setup script
# ------------------------
$minion = <<-'MINION'
  # Install the salt-minion
  apt install salt-minion -y

  # Configure Master contact details and hostname for the minion
  echo "master: 192.168.88.100" >> /etc/salt/minion
  echo "id: $(hostname)" >> /etc/salt/minion

  # Enable and restart service for configs to take place
  systemctl enable salt-minion && systemctl restart salt-minion
MINION


# Salt-Master setup script
# ------------------------
$master = <<-'MASTER'
  # Installing salt-master
  apt install salt-master -y

  # Specifying the masters ip-address to the config file and auto accepting minion-keys
  # Note: Auto accepting should only be done in test/dev environments, not for production!
  echo "interface: 192.168.88.100" >> /etc/salt/master
  echo "auto_accept: True" >> /etc/salt/master

  # Creating the file_roots path for salt and telling it where to look
  mkdir -p /srv/salt
  echo "file_roots:" >> /etc/salt/master.d/file_roots.conf
  echo "  base:" >> /etc/salt/master.d/file_roots.conf
  echo "    - /srv/salt" >> /etc/salt/master.d/file_roots.conf

  # Ensuring correct permissions
  chown root:root /srv/salt
  chmod 755 /srv/salt

  # Doing the same for pillar
  mkdir -p /srv/pillar
  echo "pillar_roots:" >> /etc/salt/master.d/pillar_roots.conf
  echo "  base:" >> /etc/salt/master.d/pillar_roots.conf
  echo "    - /srv/pillar" >> /etc/salt/master.d/pillar_roots.conf

  # Ensuring correct permissions
  chown root:root /srv/pillar
  chmod 755 /srv/pillar

  # Enable and restart for configs to take place
  systemctl enable salt-master && systemctl restart salt-master
MASTER



# Vagrant configuration for the VMS
# ----------------------------------- 

Vagrant.configure("2") do |config|
	
	# Using Bento's ubuntu 24.04 for the base box
        # Find the box that best suits your needs at:
	# https://portal.cloud.hashicorp.com/vagrant/discover
	config.vm.box = "bento/ubuntu-24.04"

        # Disable VBox shared folder (using rsync instead)
        config.vm.synced_folder ".", "/vagrant", disabled: true
	
	# Syncing /files folder from the repo to vms
	config.vm.synced_folder "./files", "/home/vagrant/files", type: "rsync", rsync__auto: true

	# Virtualbox specific configurations for VMs
	config.vm.provider "virtualbox" do |vb|
	  vb.linked_clone = true	# Use linked for more agile/speedy VM creation
	  vb.memory = 1024		# Set 1024mb of RAM
	  vb.cpus = 2			# Number of CPUs = 2
	end

	
	# Defining the Salt-master
	# ------------------------
	config.vm.define "master" do |master|
	  master.vm.hostname = "master"
	  master.vm.network "private_network", ip: "192.168.88.100"

          # Syncing salt & pillar directories to salt-master
          # Use rsync instead of VBoxAdditions syncing
          master.vm.synced_folder "./salt", "/srv/salt", type: "rsync", rsync__auto: true
	  master.vm.synced_folder "./pillar", "/srv/pillar", type: "rsync", rsync__auto: true

	  # Provision the master VM
	  master.vm.provision "shell", inline: $initscript
          master.vm.provision "shell", inline: $salt_install_apt
	  master.vm.provision "shell", inline: $master
	end


	# Defining Salt-minions
        # ---------------------
	minions = {
	"devbox" => "192.168.88.101",
	"web-serv" => "192.168.88.102",
	"db-serv" => "192.168.88.103"
	}

	minions.each do |name, ip|
	  config.vm.define name do |minion|

	    # Configure names and IP-addressing
	    minion.vm.hostname = name
	    minion.vm.network "private_network", ip: ip

	    # Provision each minion VM
	    minion.vm.provision "shell", inline: $initscript
            minion.vm.provision "shell", inline: $salt_install_apt
	    minion.vm.provision "shell", inline: $minion
	  end
	end
end

``` 

# Initialize
<img width="692" alt="Screenshot 2025-05-06 at 18 38 07" src="https://github.com/user-attachments/assets/c08a7c75-5d8d-4000-b35d-5e5cc233e553" /> <br> 

# Connection test
<img width="472" alt="Screenshot 2025-05-06 at 19 01 43" src="https://github.com/user-attachments/assets/44939584-aec7-41ec-a25f-b52dd98a6416" /> <br> 

# The file_roots
<img width="771" alt="Screenshot 2025-05-06 at 19 18 00" src="https://github.com/user-attachments/assets/dbedfcd1-783b-48b7-bc82-66b5b446b837" />

```
/srv/salt
â”œâ”€â”€ common
â”‚Â Â  â”œâ”€â”€ files
â”‚Â Â  â”‚Â Â  â””â”€â”€ ssh
â”‚Â Â  â”‚Â Â      â””â”€â”€ sshd_config
â”‚Â Â  â”œâ”€â”€ git.sls
â”‚Â Â  â””â”€â”€ ssh.sls
â”œâ”€â”€ firewall
â”‚Â Â  â”œâ”€â”€ dbserv.sls
â”‚Â Â  â”œâ”€â”€ devbox.sls
â”‚Â Â  â”œâ”€â”€ init.sls
â”‚Â Â  â””â”€â”€ webserv.sls
â”œâ”€â”€ lang
â”‚Â Â  â”œâ”€â”€ init.sls
â”‚Â Â  â”œâ”€â”€ java.sls
â”‚Â Â  â”œâ”€â”€ nodejs.sls
â”‚Â Â  â””â”€â”€ python.sls
â”œâ”€â”€ role
â”‚Â Â  â”œâ”€â”€ dbserv.sls
â”‚Â Â  â”œâ”€â”€ devbox.sls
â”‚Â Â  â””â”€â”€ webserv.sls
â”œâ”€â”€ service
â”‚Â Â  â”œâ”€â”€ apache
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ files
â”‚Â Â  â”‚Â Â  â”‚Â Â  â”œâ”€â”€ 000-default.conf
â”‚Â Â  â”‚Â Â  â”‚Â Â  â”œâ”€â”€ apache2.conf
â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ index.html
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ init.sls
â”‚Â Â  â”‚Â Â  â””â”€â”€ kill.sls
â”‚Â Â  â”œâ”€â”€ docker
â”‚Â Â  â”‚Â Â  â””â”€â”€ init.sls
â”‚Â Â  â”œâ”€â”€ mariadb
â”‚Â Â  â”‚Â Â  â””â”€â”€ init.sls
â”‚Â Â  â”œâ”€â”€ mysql
â”‚Â Â  â”‚Â Â  â””â”€â”€ init.sls
â”‚Â Â  â””â”€â”€ nginx
â”‚Â Â      â”œâ”€â”€ files
â”‚Â Â      â”‚Â Â  â”œâ”€â”€ default
â”‚Â Â      â”‚Â Â  â”œâ”€â”€ index.html
â”‚Â Â      â”‚Â Â  â”œâ”€â”€ nginx.conf
â”‚Â Â      â”‚Â Â  â””â”€â”€ webserv1.conf
â”‚Â Â      â”œâ”€â”€ init.sls
â”‚Â Â      â”œâ”€â”€ kill.sls
â”‚Â Â      â””â”€â”€ vhost.sls
â”œâ”€â”€ tool
â”‚Â Â  â”œâ”€â”€ build-essential.sls
â”‚Â Â  â”œâ”€â”€ htop.sls
â”‚Â Â  â”œâ”€â”€ httpie.sls
â”‚Â Â  â”œâ”€â”€ init.sls
â”‚Â Â  â”œâ”€â”€ nmap.sls
â”‚Â Â  â”œâ”€â”€ tcpdump.sls
â”‚Â Â  â””â”€â”€ tmux.sls
â”œâ”€â”€ top.sls
â””â”€â”€ user
    â”œâ”€â”€ devuser.sls
    â”œâ”€â”€ user1.sls
    â””â”€â”€ webadmin.sls
```

# Pillars
```
/srv/pillar
â”œâ”€â”€ database.sls
â”œâ”€â”€ top.sls
â””â”€â”€ user
    â”œâ”€â”€ devuser
    â”‚Â Â  â””â”€â”€ init.sls
    â”œâ”€â”€ user1
    â”‚Â Â  â””â”€â”€ init.sls
    â””â”€â”€ webadmin
        â””â”€â”€ init.sls
```
# Q: What are we looking at?
You're looking at a modular and scalable architecture designed for flexibility and clarity. <br> 
Its clear and consistent structure allows for easy expansion, making it simple to configure different machines/minions for various roles and purposes. <br>
By embracing a modular approach, this setup enhances maintainability and supports dynamic, streamlined system management across the entire environment.
<br> 

The environment contains in essence, for the time being:
- A database server
- A web server 
- A development instance
- The master to control them all
- A lot of room for tailoring and some additional modules/states which are excluded from `highstate`

<br> 

**Q:** What is a "`highstate`"? <br> 
**A:** Highstate --> Applying a set of predefined configurations a.k.a states, to a system, based on mappings in the `top.sls` file, to form a complete definition of a host. <br> 

# Define and control numerous machines by creating and assigning roles
**Step 1:** Create a role in the `role` module (`/srv/salt/role`) (let's take for example devbox). <br> 
**Step 2:** Define what states the role should have.
```
# /srv/salt/role/devbox.sls

# - Define the developer workstation role -

include:
  - lang
  - tool
  - service.docker
  - common.ssh
  - common.git
``` 
<br> 

**Step 3:** Map the role to desired minion using the `top.sls` file.

```
# /srv/salt/top.sls

base:

  # Define roles for minions
  # ------------------------
  'devbox':
    - role.devbox
  'web-serv':
    - role.webserv
  'db-serv':
    - role.dbserv
```

<br> 

Now you're able to run the `highstate` for minion(s) with:

```
$ sudo salt '<specify-minion(s)>' state.apply
``` 
Which will then apply all configurations specified within `/role` using `top.sls` <br>  
If you want to know how the current state would change before actually applying it (using `devbox` again for example), simply:
```
$ sudo salt 'devbox' state.apply test=true
```
<img width="913" alt="Screenshot 2025-05-06 at 19 43 03" src="https://github.com/user-attachments/assets/3c8db147-a9ce-460b-99b7-7640858b6a03" /> <br> 
### --- some output retracted ---
<img width="674" alt="Screenshot 2025-05-06 at 19 44 23" src="https://github.com/user-attachments/assets/bebe1321-0835-4178-8388-47ce01aeb177" /> <br> 

After careful consideration that this is something we actually want to do, <br> 
we can just go ahead and remove the `test=true` training wheels and run the same command again. <br> 

```
Summary for devbox
-------------
Succeeded: 16 (changed=8)
Failed:     0
-------------
Total states run:  16
Total run time:    140.004S
-------------
```
### Want to create a user for the devbox?
Just create it in the user module and apply it!

```
# /srv/salt/user/devuser.sls

# NOTE: Remember to configure password in /srv/pillar/user/devuser/init.sls
devuser:
  user.present:
    - fullname: Developer
    - shell: /bin/bash
    - home: /home/devuser
    - createhome: true
    - groups:
      - sudo 
      - docker
    - password: {{ salt['pillar.get']('password', '') }}

```

```
$ sudo salt 'devbox' state.apply user.devuser
```
```
Summary for devbox
------------
Succeeded: 1  (changed=1)
Failed:    0
------------
Total states run:      1
Total run time:  129.321 ms
```
Log into devbox with SSH and switch users to confirm a successful operation <br> 
<img width="975" alt="Screenshot 2025-05-09 at 18 50 37" src="https://github.com/user-attachments/assets/92703b04-1420-4406-8141-9c1c81c3e7c9" /> <br> 

It correctly set the password which we have configured using pillars in `/srv/pillar/user/devuser` <br> 
```
# /srv/pillar/user/devuser/init.sls

# CHANGE THIS PASSWORD!!!

password: "$6$FOL3kWj5uW834DPY$HQuWsMVdq...<rest_of_hash>"
```
I just hashed a weak password to demonstrate that it works. Hence the comment "change password". <br> 
For the hashing operation I used:
```
$ openssl passwd -6 '<insert_your_password_here>'
``` 
**Side note:** the hashing is a requirement for this to work! <br> 
Also remember to map it correctly in the `top.sls` file.

```
# /srv/salt/pillar/top.sls

base:

  '*':
    - user.user1

  'db-serv':
    - database

  'devbox':
    - user.devuser

  'web-serv':
    - user.webadmin
```

### Q: But is it idempotent? 
Running the same states again. First the highstate and then the user. (user can be included in highstate but i chose not to for demonstrative purposes):
```
Comment: All specified packages are already installed
```
```
Comment: The service docker is already running
```
```
Comment: File /etc/ssh/sshd_config is in the correct state
```

```
Summary for devbox
-------------
Succeeded: 16
Failed:     0
-------------
Total states run:  16
Total run time:    99.521 ms
``` 

```
devbox:
----------
          ID: devuser
    Function: user.present
      Result: True
     Comment: User devuser is present and up to date
     Started: 16:05:18.841933
    Duration: 24.034 ms
     Changes:   

Summary for devbox
------------
Succeeded: 1
Failed:    0
------------
Total states run:     1
Total run time:  24.034 ms
```


### Setting up the web server
```
$ sudo salt 'web-serv' state.apply
```
```
----------
	  ID: apache_service
    Function: service.running
        Name: apache2
      Result: True
     Comment: The service apache2 is already running
     Started: 17:01:03.413864
    Duration: 22.702 ms
    Changes :
----------
          ID: /var/www/html/index.html
    Function: file.managed
      Result: True
     Comment: File /var/www/html/index.html updated
     Started: 17:01:03.439355
    Duration: 63.177 ms
     Changes:
	      ----------
	      diff:
                  ---
                  +++
		  @@ -1,236 +1,45 @@
```
```
Summary for web-serv
------------
Succeeded: 6 (changed=4)
Failed:    0
------------
Total states run:     6
Total run time:  83.438 s
```

### Making sure changes were actually applied
Apparently `index.html` file was updated and the service is runnning. <br> 
Lets's make sure of this by typing our web-servers ip-address into the browser and see what happens. <br> 
![Screenshot 2025-05-06 at 20 09 26](https://github.com/user-attachments/assets/b7273105-4b03-4d1c-a05c-244b3c2b81e0) <br> 
Once again we have a successful operation. <br> 
The default page has disappeared and we have our very own "homepage" ðŸ˜‚.


# Seamless Server Switching
Implementing a dynamic approach to web server management is easy with Salt. <br> 

Let me introduce you to the nginx module:
```
nginx
â”œâ”€â”€ files
â”‚Â Â  â”œâ”€â”€ default
â”‚Â Â  â”œâ”€â”€ index.html
â”‚Â Â  â”œâ”€â”€ nginx.conf
â”‚Â Â  â””â”€â”€ webserv1.conf
â”œâ”€â”€ init.sls
â”œâ”€â”€ kill.sls
â””â”€â”€ vhost.sls
```
`init.sls` = Install the software, enable the service, manage config files and give our `webadmin` user necessary privileges. <br> 
<br> 
By configuring both Apache and Nginx as separate modules and using 
`service.<service>.kill` within each moduleâ€™s `init.sls` file, we ensure that one server is gracefully stopped before the other is started. <br>  
This mechanism allows seamless switching between web servers, reducing conflicts and ensuring only one service runs at a time!
```
# /srv/salt/service/nginx/init.sls

include:
  - service.apache.kill

# Install nginx
install_nginx:
  pkg.installed:
    - name: nginx


# Enable and run
nginx_service:
  service.running:
    - name: nginx
    - enable: True
    - watch: 
      - file: /etc/nginx/nginx.conf
      - file: /etc/nginx/sites-available/default
    - require:
        - pkg: nginx


# Managing nginx config file
/etc/nginx/nginx.conf:
  file.managed:
    - source: salt://service/nginx/files/nginx.conf
    - user: root
    - group: root
    - mode: 640


# Managing the index file
/var/www/html/index.html:
  file.managed:
    - source: salt://service/nginx/files/index.html
    - user: root
    - group: root
    - mode: 644


# Managing the default file
default_available:
  file.managed:
    - name: /etc/nginx/sites-available/default
    - source: salt://service/nginx/files/default
    - user: root
    - group: root
    - mode: 640 


# Enabling the site by symlinking it to sites-enabled
/etc/nginx/sites-enabled/default:
  file.symlink:
    - target: /etc/nginx/sites-available/default
    - require:
      - file: default_available
```
If you look through the `init.sls` file, you'll notice we use `salt://` as a source quite a bit. All necessary files are in the same module. Ensuring a dynamic and modular approach. 
```
nginx/files
â”œâ”€â”€ default
â”œâ”€â”€ index.html
â”œâ”€â”€ nginx.conf
â””â”€â”€ webserv1.conf
```
We can easily set up a basic nginx webserver with this setup, but why stop there? Let's put up a virtualhost configuration as well.

### Introducing vhost.sls
```
# /srv/salt/service/nginx/vhost.sls

include:
  - user.webadmin
  - service.nginx


# Manage the virtual host config file
/etc/nginx/sites-available/webserv1:
  file.managed:
    - source: salt://service/nginx/files/webserv1.conf
    - user: root
    - group: root
    - mode: 644
    - require:
      - pkg: nginx


# Enable site by symlinking it to sites-enabled 
/etc/nginx/sites-enabled/webserv1:
  file.symlink:
    - target: /etc/nginx/sites-available/webserv1
    - force: True
    - require:
      - file: /etc/nginx/sites-available/webserv1


# Manage document roots
/var/www/webserv1/html:
  file.directory:
    - user: webadmin
    - group: www-data
    - mode: 755
    - makedirs: True
    - recurse:
      - user
      - group
    - require:
      - user: webadmin


# Deploy and manage initial index.html
/var/www/webserv1/html/index.html:
  file.managed:
    - source: salt://service/nginx/files/index.html
    - user: webadmin
    - group: www-data
    - mode: 644
    - require: 
      - file: /var/www/webserv1/html


# Reload nginx upon changes to virtual host config
nginx:
  service.running:
    - reload: True
    - watch:
      - file: /etc/nginx/sites-available/webserv1
```
The `webserv1.conf` file we're referring to:
```
# Virtual Host configuration
#
server {
       listen 80;
       listen [::]:80;

       server_name nginxwebserv1.com www.nginxwebserv1.com;

       root /var/www/webserv1/html;
       index index.html;

       location / {
               try_files $uri $uri/ =404;
       }
  }
```
The `webadmin` user
```
# /srv/salt/user/webadmin.sls

webadmin_user:
  user.present:
    - name: webadmin
    - home: /home/webadmin
    - shell: /bin/bash
    - createhome: True
    - password: {{ salt['pillar.get']('password', '') }}
    - groups:
      - www-data
      - sudo
```

### Switcheroo
Now we can either change the mapping in our `role` module for the `web-serv` from `service.apache` to  `service.nginx` and run the `highstate`. <br> 
Or we can simply run `$ sudo salt 'web-serv' state.apply service.nginx.vhost` (because of the "include" clause in our `vhost.sls` file, it will run the `nginx.init` state first, ensuring nginx is installed & running). <br> 
Here's a snippet from running the state:
```
----------
          ID: webadmin_user
    Function: user.present
        Name: webadmin
      Result: True
     Comment: User webadmin is present and up to date
     Started: 09:05:04.147327
    Duration: 25.384 ms
     Changes:   
----------
          ID: kill_apache
    Function: service.dead
        Name: apache2
      Result: True
     Comment: The service apache2 is already dead
     Started: 09:05:04.178404
    Duration: 26.329 ms
     Changes:   
----------
          ID: install_nginx
    Function: pkg.installed
        Name: nginx
      Result: True
     Comment: All specified packages are already installed
     Started: 09:05:04.518164
    Duration: 15.856 ms
     Changes:   
----------
----------
          ID: /etc/nginx/sites-available/webserv1
    Function: file.managed
      Result: True
     Comment: File /etc/nginx/sites-available/webserv1 is in the correct state
     Started: 09:09:02.294570
    Duration: 8.393 ms
     Changes:   
----------

```
As the module I have created is completely idempotent, we will get the following summary after subsequent runs:
```
Summary for web-serv
-------------
Succeeded: 13
Failed:     0
-------------
Total states run:     13
Total run time:  153.668 ms
```
### Did it work? 
![Screenshot 2025-05-14 at 12 12 19](https://github.com/user-attachments/assets/76dea759-9626-4d17-a55f-9701cb1567ad) <br> 
**Peep out the domain name.** ðŸ‘€ 
**Note:** Remember to configure the `/etc/hosts` file on your host machine if you wan't to use a dn.


### Reshaping the server on the go
Want to go back to using Apache? Well luckily for you, that's easy! <br> 
Just:
```
$ sudo salt 'web-serv' state.apply service.apache
```
**Note:** Using `state.apply` by itself, or appending `service.apache` depends on which service you have mapped in your `role` module for the `web-serv`.
```
vagrant@master:~$ sudo salt 'web-serv' state.apply service.apache
web-serv:
----------
          ID: kill_nginx
    Function: service.dead
        Name: nginx
      Result: True
     Comment: Service nginx has been disabled, and is dead
     Started: 09:18:39.189627
    Duration: 540.385 ms
     Changes:   
              ----------
              nginx:
                  True
----------
          ID: webadmin_user
    Function: user.present
        Name: webadmin
      Result: True
     Comment: User webadmin is present and up to date
     Started: 09:18:39.730780
    Duration: 46.231 ms
     Changes:   
----------
----------
          ID: /etc/apache2/sites-available/000-default.conf
    Function: file.managed
      Result: True
     Comment: File /etc/apache2/sites-available/000-default.conf is in the correct state
     Started: 09:18:40.130339
    Duration: 8.13 ms
     Changes:   
----------
          ID: apache_service
    Function: service.running
        Name: apache2
      Result: True
     Comment: Service apache2 has been enabled, and is running
     Started: 09:18:40.138553
    Duration: 435.979 ms
     Changes:   
              ----------
              apache2:
                  True
----------

Summary for web-serv
------------
Succeeded: 9 (changed=4)
Failed:    0
------------
Total states run:     9
Total run time:   1.069 s
```
Because we changed back to Apache, we will ofcourse get some changes. But running it again proves idempotency:
```
Summary for web-serv
------------
Succeeded: 9
Failed:    0
------------
Total states run:     9
Total run time: 108.973 ms
```
Now we can access the `index.html` provided by Apache again:
![Screenshot 2025-05-14 at 12 16 15](https://github.com/user-attachments/assets/633273f3-2715-4a4c-a860-6ce967d19fd6) <br> 



# Setting up the database-server
**init.sls:** 
```
# /srv/salt/service/mariadb/init.sls


# Necessary dependency install 
debconf-utils:
  pkg.installed


# Preseed the root password. 
set_mariadb_root:
  debconf.set:
    - name: mariadb-server
    - data:
        'mysql-server/root_password': {'type': 'password', 'value': {{ salt['pillar.get']('mysql_root', '') }} }
        'mysql-server/root_password_again': {'type': 'password', 'value': {{ salt['pillar.get']('mysql_root', '') }} }
    - require:
      - pkg: debconf-utils

# Install mariadb
#
mariadb_install:
  pkg.installed:
    - pkgs:
      - mariadb-server
      - mariadb-client
    - require:
      - debconf: set_mariadb_root

# Ensure service is running
#
mariadb_service:
  service.running:
    - name: mariadb
    - enable: True
    - require:
      - pkg: mariadb_install
```

### Using pillars for the password
**database.sls:** 
```
# /srv/salt/pillar/database.sls

# NOTE: CHANGE THIS PASSWORD!!!

mysql_root: "$6$aQ4zg/Y8lpkLhw7w$uwKcyuqoyYf4Fyv...<rest_of_hash>"
```
**top.sls** 
```
# /srv/salt/pillar/top.sls

base:
  '*':
    - user.user1

  'db-serv':
    - database

  'devbox':
    - user.devuser

  'web-serv':
    - user.webadmin
```

### Trial run
Running a test to see if we are reaching the pillar correctly
```
$ sudo salt 'db-serv' state.apply test=true
```

```
----------
          ID: set_mariadb_root
    Function: debconf.set
        Name: mariadb-server
      Result: None
     Comment: 
     Started: 13:55:26.422040
    Duration: 90.097 ms
     Changes:   
              ----------
              mysql-server/root_password:
                  New value: <retracted_by_author>
              mysql-server/root_password_again:
                  New value: <retracted_by_author>
----------
```

```
Summary for db-serv
------------
Succeeded: 6 (unchanged=3, changed=2)
Failed:    0
------------
Total states run:     6
Total run time:   2.286 s
```

### Removing training wheels
```
$ sudo salt 'db-serv' state.apply
```

This time around we see pillars in action (password hidden)
```
----------
          ID: set_mariadb_root
    Function: debconf.set
        Name: mariadb-server
      Result: True
     Comment: 
     Started: 13:58:33.997710
    Duration: 98.715 ms
     Changes:   
              ----------
              mysql-server/root_password:
                  (password hidden)
              mysql-server/root_password_again:
                  (password hidden)
```

```
----------
          ID: mariadb_service
    Function: service.running
        Name: mariadb
      Result: True
     Comment: The service mariadb is already running
     Started: 17:45:07.253788
    Duration: 14.322 ms
     Changes:   
----------
```
```
Summary for db-serv
------------
Succeeded: 6 (changed=3)
Failed:    0
------------
Total states run:     6
Total run time:  32.138 s
```
### Idempotency?
Let's run the same command again and see what happens...
```
----------
          ID: set_mariadb_root
    Function: debconf.set
        Name: mariadb-server
      Result: True
     Comment: All specified answers are already set
     Started: 17:58:26.466594
    Duration: 35.99 ms
     Changes:   
----------
```
```
----------
          ID: mariadb_service
    Function: service.running
        Name: mariadb
      Result: True
     Comment: The service mariadb is already running
     Started: 17:58:26.507240
    Duration: 20.512 ms
     Changes:   
----------
```
```
Summary for db-serv
------------
Succeeded: 6
Failed:    0
------------
Total states run:     6
Total run time:  82.648 ms
```
### And then the final test: Logging into the database locally from the db-server. 
```
$ ssh vagrant@192.168.88.103
```
```
vagrant@db-serv:~$ sudo mariadb -u root -p
Enter password: 
Welcome to the MariaDB monitor.  Commands end with ; or \g.
Your MariaDB connection id is 36
Server version: 10.11.11-MariaDB-0ubuntu0.24.04.2 Ubuntu 24.04

Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

MariaDB [(none)]>
```
Typed in the password I configured and got access to the database. Everything seems working. 

# Final thoughts
This environment was created and configured with modularity, portability and customisation in mind. <br> 
I have tried to comment everything for clarity and readability. <br> 
The idea is that anyone who uses this template for their configuration management is able to quickly get a hang of it and start using it as their own. <br> 
**Quick mention:** As i'm writing this, it is still a work in progress and have a few improvements in mind. <br> 
Updates coming up...

## Stay tuned! 
<br> 

# "It works on my computer"
- Vagrant 2.4.5
- VirtualBox 7.1.4 
- MacBook Pro M2 2022 - 8GB RAM - Sequoia 15.4.1

<br> 

# Some light reading & a video for those interested
Salt Essentials (book) by Craig Sebenik & Thomas Hatch (2015) <br> 
Salt bootsrap - <https://docs.saltproject.io/en/3006/topics/tutorials/salt_bootstrap.html> <br> 
Salt bootstrap download - <https://github.com/saltstack/salt-bootstrap?tab=readme-ov-file#testing-in-vagrant> <br> 
Salt documentation - <https://docs.saltproject.io/en/latest/contents.html> <br> 
Salt pillars - <https://docs.saltproject.io/salt/user-guide/en/latest/topics/pillar.html> <br> 
MySQL preseed - <https://terokarvinen.com/2018/mysql-automatic-install-with-salt-preseed-database-root-password/> <br> 
MySQL state - <https://www.digitalocean.com/community/tutorials/saltstack-infrastructure-creating-salt-states-for-mysql-database-servers> <br> 
Nginx Salt States - <https://www.digitalocean.com/community/tutorials/saltstack-infrastructure-creating-salt-states-for-nginx-web-servers> <br>
Nginx VirtualHost - <https://www.digitalocean.com/community/tutorials/how-to-set-up-nginx-server-blocks-virtual-hosts-on-ubuntu-16-04> <br>
Apache VirtualHost - <https://httpd.apache.org/docs/2.4/vhosts/examples.html> <br>
Firewalld - <https://docs.saltproject.io/en/latest/ref/states/all/salt.states.firewalld.html#salt.states.firewalld.service> <br> 
SaltStack Configuration Management Best Practices - <https://www.youtube.com/watch?v=RbXnXZu_4ng>







